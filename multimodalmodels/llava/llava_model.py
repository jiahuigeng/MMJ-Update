import os
import torch
from torchvision import transforms
from torch.nn import CrossEntropyLoss
from torchvision import transforms
from transformers import AutoProcessor, LlavaForConditionalGeneration
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
from transformers import BatchFeature
from ..multimodalmodel import MultiModalModel
from PIL import Image


class LLaVA_v1_5(MultiModalModel):
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-7b-hf").to('cuda')
        self.processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")
        self.normalize = transforms.Normalize(mean=self.processor.image_processor.image_mean, std=self.processor.image_processor.image_std)
        self.preprocess = lambda x: self.processor.image_processor.preprocess(x, do_normalize=False, return_tensors='pt')['pixel_values'][0]
        self.tokenizer_kwargs = {"padding": False, "truncation": None, "max_length": None}
        self.vis_processor = self.processor.image_processor

    def generate(self, test_cases, image_dir, **generation_kwargs):
        generation_kwargs.setdefault('do_sample', False)
        generation_kwargs.setdefault('num_beams', 1)
        assert generation_kwargs['do_sample'] is False, "do_sample should be False"
        assert generation_kwargs['num_beams'] == 1, "num_beams should be 1"
        outputs = []
        for case in test_cases:
            image, prompt = case
            if isinstance(image, str):
                raw_image = Image.open(os.path.join(image_dir, image))

            conv_prompt = f"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n{prompt} ASSISTANT:"

            inputs = self.processor(conv_prompt, raw_image, return_tensors='pt').to(self.device)
            output = self.model.generate(**inputs, **generation_kwargs)
            output = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
            outputs.append(output.rsplit('ASSISTANT:', 1)[-1].strip())

        return outputs

    def query(self, image, prompt):
        input = self.processor(text=f"Human: {prompt} <image> Assistant: ", images=image, return_tensors="pt").to(self.device)
        output = self.model.generate(**input,max_length=300)
        outnpy = output.to("cpu").numpy()
        answer = self.processor.decode(outnpy[0], skip_special_tokens=True)
        answer = answer.split('Assistant: ')[-1].strip()
        return answer

    def compute_loss(self, behavior, target, image_input):
        conv_prompt = f"<image>\nUSER: {behavior}\nASSISTANT: {target}"
        text_inputs = self.processor.tokenizer(conv_prompt, return_tensors='pt', **self.tokenizer_kwargs).to(self.device)
        pixel_values = self.normalize(image_input).unsqueeze(0).to(self.device)
        inputs = BatchFeature(data={**text_inputs, "pixel_values": pixel_values})
        model_inputs = self.model.prepare_inputs_for_generation(**inputs)

        target_ids = self.processor.tokenizer(target, return_tensors='pt', add_special_tokens=False, **self.tokenizer_kwargs)['input_ids'].to(self.device)
        assert text_inputs['input_ids'][0][-len(target_ids[0]):].equal(target_ids[0]), "Text inputs do not end with target ids"

        outputs = self.model(**model_inputs)
        shift_logits = outputs.logits[0, -(target_ids.size(1)+1):-1, :]
        loss_fn = CrossEntropyLoss()
        return loss_fn(shift_logits, target_ids.view(-1))

    def compute_loss_batch(self, x_adv, batch_targets):
        batch_size = len(batch_targets)
        input_ids = []
        labels = []
        images = []

        template =  "A chat between a curious user and an artificial intelligence assistant. \
        The assistant gives helpful, detailed, and polite answers to the user's questions. \
        USER: <image>\n ASSISTANT: {target}</s>"
        
        for target in batch_targets:
            text = template.format(target=target)
            input_id = self.processor.tokenizer(text, return_tensors = "pt")['input_ids'][0]
            label = input_id.clone()

            sep = " ASSISTANT: "
            parts = text.split(sep)
            parts[0] += sep

            instruction_len = len(self.processor.tokenizer(parts[0], return_tensors='pt')[0]) - 1
            label[:instruction_len] = -100

            input_ids.append(input_id)
            labels.append(label)               

        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids,
            batch_first=True,
            padding_value=self.processor.tokenizer.pad_token_id)
        labels = torch.nn.utils.rnn.pad_sequence(labels,
                                                    batch_first=True,
                                                    padding_value=-100)
        input_ids = input_ids[:, :self.processor.tokenizer.model_max_length].to(self.device)
        labels = labels[:, :self.processor.tokenizer.model_max_length].to(self.device)
        attention_mask = input_ids.ne(self.processor.tokenizer.pad_token_id).to(self.device)

        images = [x_adv] * batch_size

        pixel_values = torch.concat(images, axis = 0).to(self.device)
            
        loss = self.model(
            input_ids = input_ids,
            attention_mask = attention_mask,
            labels = labels,
            pixel_values = pixel_values
        ).loss

        return loss

class LLaVA_v1_5L(MultiModalModel):
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-13b-hf").to('cuda')
        self.processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-13b-hf")
        self.normalize = transforms.Normalize(mean=self.processor.image_processor.image_mean, std=self.processor.image_processor.image_std)
        self.preprocess = lambda x: self.processor.image_processor.preprocess(x, do_normalize=False, return_tensors='pt')['pixel_values'][0]
        self.tokenizer_kwargs = {"padding": False, "truncation": None, "max_length": None}
        self.vis_processor = self.processor.image_processor

    def generate(self, test_cases, image_dir, **generation_kwargs):
        generation_kwargs.setdefault('do_sample', False)
        generation_kwargs.setdefault('num_beams', 1)
        assert generation_kwargs['do_sample'] is False, "do_sample should be False"
        assert generation_kwargs['num_beams'] == 1, "num_beams should be 1"
        outputs = []
        for case in test_cases:
            image, prompt = case
            if isinstance(image, str):
                raw_image = Image.open(os.path.join(image_dir, image))

            conv_prompt = f"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n{prompt} ASSISTANT:"

            inputs = self.processor(conv_prompt, raw_image, return_tensors='pt').to(self.device)
            output = self.model.generate(**inputs, **generation_kwargs)
            output = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
            outputs.append(output.rsplit('ASSISTANT:', 1)[-1].strip())

        return outputs

    def query(self, image, prompt):
        input = self.processor(text=f"Human: {prompt} <image> Assistant: ", images=image, return_tensors="pt").to(self.device)
        output = self.model.generate(**input,max_length=300)
        outnpy = output.to("cpu").numpy()
        answer = self.processor.decode(outnpy[0], skip_special_tokens=True)
        answer = answer.split('Assistant: ')[-1].strip()
        return answer

    def compute_loss(self, behavior, target, image_input):
        conv_prompt = f"<image>\nUSER: {behavior}\nASSISTANT: {target}"
        text_inputs = self.processor.tokenizer(conv_prompt, return_tensors='pt', **self.tokenizer_kwargs).to(self.device)
        pixel_values = self.normalize(image_input).unsqueeze(0).to(self.device)
        inputs = BatchFeature(data={**text_inputs, "pixel_values": pixel_values})
        model_inputs = self.model.prepare_inputs_for_generation(**inputs)

        target_ids = self.processor.tokenizer(target, return_tensors='pt', add_special_tokens=False, **self.tokenizer_kwargs)['input_ids'].to(self.device)
        assert text_inputs['input_ids'][0][-len(target_ids[0]):].equal(target_ids[0]), "Text inputs do not end with target ids"

        outputs = self.model(**model_inputs)
        shift_logits = outputs.logits[0, -(target_ids.size(1)+1):-1, :]
        loss_fn = CrossEntropyLoss()
        return loss_fn(shift_logits, target_ids.view(-1))

    def compute_loss_batch(self, x_adv, batch_targets):
        batch_size = len(batch_targets)
        input_ids = []
        labels = []
        images = []

        template =  "A chat between a curious user and an artificial intelligence assistant. \
        The assistant gives helpful, detailed, and polite answers to the user's questions. \
        USER: <image>\n ASSISTANT: {target}</s>"
        
        for target in batch_targets:
            text = template.format(target=target)
            input_id = self.processor.tokenizer(text, return_tensors = "pt")['input_ids'][0]
            label = input_id.clone()

            sep = " ASSISTANT: "
            parts = text.split(sep)
            parts[0] += sep

            instruction_len = len(self.processor.tokenizer(parts[0], return_tensors='pt')[0]) - 1
            label[:instruction_len] = -100

            input_ids.append(input_id)
            labels.append(label)               

        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids,
            batch_first=True,
            padding_value=self.processor.tokenizer.pad_token_id)
        labels = torch.nn.utils.rnn.pad_sequence(labels,
                                                    batch_first=True,
                                                    padding_value=-100)
        input_ids = input_ids[:, :self.processor.tokenizer.model_max_length].to(self.device)
        labels = labels[:, :self.processor.tokenizer.model_max_length].to(self.device)
        attention_mask = input_ids.ne(self.processor.tokenizer.pad_token_id).to(self.device)

        images = [x_adv] * batch_size

        pixel_values = torch.concat(images, axis = 0).to(self.device)
            
        loss = self.model(
            input_ids = input_ids,
            attention_mask = attention_mask,
            labels = labels,
            pixel_values = pixel_values
        ).loss

        return loss

        



class LLaVA_v1_6(MultiModalModel):
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = LlavaNextForConditionalGeneration.from_pretrained('llava-v1.6-vicuna-7b-hf').to(self.device)
        self.processor = LlavaNextProcessor.from_pretrained('llava-v1.6-vicuna-7b-hf')

    def generate(self, test_cases, image_dir, **generation_kwargs):
        generation_kwargs.setdefault('do_sample', False)
        generation_kwargs.setdefault('num_beams', 1)
        assert generation_kwargs['do_sample'] is False, "do_sample should be False"
        assert generation_kwargs['num_beams'] == 1, "num_beams should be 1"
        outputs = []
        for case in test_cases:
            image, prompt = case
            if isinstance(image, str):
                raw_image = Image.open(os.path.join(image_dir, image))
                output = self.query(raw_image, prompt)
                outputs.append(output)

        return outputs 

    def query(self, image, prompt):
        text = f"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n{prompt} ASSISTANT:"
        inputs = self.processor(text=text, images = image, return_tensors="pt").to(self.device)
        output = self.model.generate(**inputs, max_new_tokens=512)
        answer = self.processor.decode(output[0], skip_special_tokens=True)
        answer = answer.split('ASSISTANT:')[-1].strip()
        return answer

    
    def compute_loss(self, behavior, target, image_input):
        return super().compute_loss(behavior, target, image_input)
    
    def compute_loss_batch(self, x_adv, batch_targets):
        pass