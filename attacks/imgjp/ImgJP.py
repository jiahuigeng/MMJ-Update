import os
import json
from tqdm import tqdm

from transformers import StoppingCriteriaList, TextIteratorStreamer

from attacks.xidian.utils.minigpt4.common.config_1 import Config
from attacks.xidian.utils.minigpt4.common.dist_utils import get_rank
from attacks.xidian.utils.minigpt4.common.registry import registry
from attacks.xidian.utils.minigpt4.conversation.conversation import Chat, CONV_VISION_Vicuna0, CONV_VISION_LLama2, StoppingCriteriaSub


from attacks.xidian.utils.torchattacks.attacks.pixle import *
from attacks.xidian.utils.torchattacks.attacks.bim import *
from attacks.xidian.utils.torchattacks.attacks.pgd_uap_v1 import *
from attacks.xidian.utils.torchattacks.attacks.pgdl2 import *

import torch.backends.cudnn as cudnn
import random
import time
import torch
import numpy as np
import torch.nn as nn

import csv

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def save_image(image_array: np.ndarray, f_name: str) -> None:
    """
    Saves image into a file inside `ART_DATA_PATH` with the name `f_name`.

    :param image_array: Image to be saved.
    :param f_name: File name containing extension e.g., my_img.jpg, my_img.png, my_images/my_img.png.
    """
    from PIL import Image
    image = Image.fromarray(image_array)
    image.save(f_name)


def denorm(image):
    mean = (0.48145466, 0.4578275, 0.40821073)
    std = (0.26862954, 0.26130258, 0.27577711)
    mean = torch.tensor(mean).to(device)
    std = torch.tensor(std).to(device)

    image_denorm = image * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)

    return image_denorm


class ImgJP:
    def __init__(self, **kwargs):
        self.train_num = kwargs['train_num']
        self.eps = kwargs['eps']
        self.alpha = kwargs['alpha']
        self.steps = kwargs['steps']
        self.random_start = kwargs['random_start']


    def generate(self, save_dir):

        random_number = random.randint(1, 2000)
        random.seed(random_number)
        np.random.seed(random_number)
        torch.manual_seed(random_number)

        model = MiniGPT(self.train_num)
        model = model.eval()
        attack = PGD(model, eps=self.eps / 255, alpha=self.alpha / 255, steps=self.steps, nprompt=model.train_num,
                 random_start=False)
        
        attack.set_mode_targeted_by_label()
        mean = (0.48145466, 0.4578275, 0.40821073)
        std = (0.26862954, 0.26130258, 0.27577711)
        attack.set_normalization_used(mean, std)

        image = torch.zeros(1, 3, 224, 224).to(device)
        images = []
        images.append(image)
        adv_img = attack(images, model.shift_labels_1)
        adv_image = denorm(adv_img[0])
        save_img = (adv_image[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
        save_image(save_img, '%s/bad_prompt.bmp' % save_dir)

    def generate_test_cases(self, behaviors, save_dir, verbose=False):
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(os.path.join(save_dir, 'images'), exist_ok=True)    

        test_cases = {}

        image_path = os.path.join(save_dir, 'images')
        self.generate(image_path)

        for behavior_dict in tqdm(behaviors, total = len(behaviors)):            
            behavior = behavior_dict['Behavior']
            behavior_id = behavior_dict['BehaviorID']
            if verbose:
                print(f"Behavior: {behavior}")
                print(f"Behavior ID: {behavior_id}")

            current_test_case = ['bad_prompt.bmp', behavior]
            test_cases[behavior_id] = current_test_case

        save_path = os.path.join(save_dir, 'Xidian.json')
        os.makedirs(os.path.dirname(save_path), exist_ok=True) if os.path.dirname(save_path) else None
        with open(save_path, 'w') as file:
            json.dump(test_cases, file, indent=2)        


class MiniGPT(nn.Module):
    def __init__(self, train_num):
        super(MiniGPT, self).__init__()
        random_number = random.randint(1, 2000)
        random.seed(random_number)
        np.random.seed(random_number)
        torch.manual_seed(random_number)
        cudnn.benchmark = False
        cudnn.deterministic = True
        conv_dict = {'pretrain_vicuna0': CONV_VISION_Vicuna0,
                     'pretrain_vicuna1': CONV_VISION_Vicuna0,
                     'pretrain_llama2': CONV_VISION_LLama2}

        cfg = Config()

        device = torch.device('cuda:0')
        self.device = device
        self.train_num = train_num

        self.mprompt = []
        self.answers = []

        rnd_idx = random.sample(range(483), self.train_num)
        self.train_goal_index = sorted(rnd_idx)
        rr = 0
        with open('./dataset/advbench/harmful_behaviors_2.csv', newline='') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                if rr in rnd_idx:
                    self.mprompt.append(row['goal'])
                    self.answers.append(row['target'])
                rr += 1

        stop_words_ids = [[835], [2277, 29937]]
        stop_words_ids = [torch.tensor(ids).to(device='cuda:{}'.format(0)) for ids in stop_words_ids]
        stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])
        if stopping_criteria is not None:
            self.stopping_criteria = stopping_criteria
        else:
            stop_words_ids = [torch.tensor([2]).to(device)]
            self.stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])
        # ========================================
        #             Model_1:Minigpt4(Vicuna-7B) Initialization
        # ========================================
        print('Minigpt4(Vicuna-7B) Initialization')
        model_config_1 = cfg.model_cfg_1
        model_config_1.device_8bit = 0
        model_cls_1 = registry.get_model_class(model_config_1.arch)
        self.model_1 = model_cls_1.from_config(model_config_1).to(device)
        vis_processor_cfg_1 = cfg.datasets_cfg_1.cc_sbu_align.vis_processor.train
        self.vis_processor_1 = registry.get_processor_class(vis_processor_cfg_1.name).from_config(vis_processor_cfg_1)
        self.model_1 = self.model_1.eval()

        CONV_VISION_1 = conv_dict[model_config_1.model_type]
        conv_1 = CONV_VISION_1.copy()
        conv_1.append_message(conv_1.roles[0], "<Img><ImageHere></Img>")
        image = torch.load('attacks/xidian/utils/images/vis_processed_merlion_minigpt4_llama.pt')
        image_emb_1, _ = self.model_1.encode_img(image)
        image_list_1 = []
        image_list_1.append(image_emb_1)

        self.q_conv_1 = []
        for test_text in self.mprompt:
            conv_tmp = conv_1.copy()
            if len(conv_tmp.messages) > 0 and conv_tmp.messages[-1][0] == conv_tmp.roles[0] \
                    and conv_tmp.messages[-1][1][-6:] == '</Img>':  # last message is image.
                conv_tmp.messages[-1][1] = ' '.join([conv_tmp.messages[-1][1], test_text])
            else:
                conv_tmp.append_message(conv_tmp.roles[0], test_text)
            conv_tmp.append_message(conv_tmp.roles[1], None)
            self.q_conv_1.append(conv_tmp)

        # self.test_conv_1 = []
        # for test_text in self.test_goal:
        #     conv_tmp = conv_1.copy()
        #     if len(conv_tmp.messages) > 0 and conv_tmp.messages[-1][0] == conv_tmp.roles[0] \
        #             and conv_tmp.messages[-1][1][-6:] == '</Img>':  # last message is image.
        #         conv_tmp.messages[-1][1] = ' '.join([conv_tmp.messages[-1][1], test_text])
        #     else:
        #         conv_tmp.append_message(conv_tmp.roles[0], test_text)
        #     conv_tmp.append_message(conv_tmp.roles[1], None)
        #
        #     self.test_conv_1.append(conv_tmp)

        self.conv_1 = []
        self.target_len_1 = []
        self.shift_labels_1 = []

        for k in range(self.train_num):
            goal = self.mprompt[k]
            target = self.answers[k]

            conv_ = conv_1.copy()

            # print(text)
            if len(conv_.messages) > 0 and conv_.messages[-1][0] == conv_.roles[0] \
                    and conv_.messages[-1][1][-6:] == '</Img>':  # last message is image.
                conv_.messages[-1][1] = ' '.join([conv_.messages[-1][1], goal])
            else:
                conv_.append_message(conv_.roles[0], goal)
            conv_.append_message(conv_.roles[1], target)
            self.conv_1.append(conv_)

            embs, inputs_tokens = self.get_context_emb(self.model_1, conv_, image_list_1, flag=1)

            target_len_ = inputs_tokens.shape[1]
            self.target_len_1.append(target_len_)

            shift_labels_ = inputs_tokens[..., 1:].contiguous()
            self.shift_labels_1.append(shift_labels_)
        print('Finished Minigpt4(Vicuna-7B) Initialization')
        # ========================================
        #             Model_2:Minigpt4(Vicuna-13B) Initialization
        # ========================================
        print('Minigpt4(Vicuna-13B) Initialization')
        model_config_2 = cfg.model_cfg_2
        model_config_2.device_8bit = 0
        model_cls_2 = registry.get_model_class(model_config_2.arch)
        self.model_2 = model_cls_2.from_config(model_config_2).to(device)
        vis_processor_cfg_2 = cfg.datasets_cfg_2.cc_sbu_align.vis_processor.train
        self.vis_processor_2 = registry.get_processor_class(vis_processor_cfg_2.name).from_config(vis_processor_cfg_2)
        self.model_2 = self.model_2.eval()
        print('Finished Minigpt4(Vicuna-13B) Initialization')
        # ========================================
        #             Model_3:Minigptv2(Llama2-7B) Initialization
        # ========================================
        print('Minigpt4(Llama2-7B) Initialization')
        model_config_3 = cfg.model_cfg_3
        model_config_3.device_8bit = 0
        model_cls_3 = registry.get_model_class(model_config_3.arch)
        self.model_3 = model_cls_3.from_config(model_config_3).to(device)
        vis_processor_cfg_3 = cfg.datasets_cfg_3.cc_sbu_align.vis_processor.train
        self.vis_processor_3 = registry.get_processor_class(vis_processor_cfg_3.name).from_config(vis_processor_cfg_3)
        self.model_3 = self.model_3.eval()

        CONV_VISION_2 = conv_dict[model_config_3.model_type]
        conv_2 = CONV_VISION_2.copy()
        conv_2.append_message(conv_2.roles[0], "<Img><ImageHere></Img>")

        image_emb_2, _ = self.model_3.encode_img(image)
        image_list_2 = []
        image_list_2.append(image_emb_2)
        self.q_conv_2 = []
        for test_text in self.mprompt:
            conv_tmp = conv_2.copy()
            if len(conv_tmp.messages) > 0 and conv_tmp.messages[-1][0] == conv_tmp.roles[0] \
                    and conv_tmp.messages[-1][1][-6:] == '</Img>':  # last message is image.
                conv_tmp.messages[-1][1] = ' '.join([conv_tmp.messages[-1][1], test_text])
            else:
                conv_tmp.append_message(conv_tmp.roles[0], test_text)
            conv_tmp.append_message(conv_tmp.roles[1], None)
            self.q_conv_2.append(conv_tmp)

        self.conv_2 = []
        self.target_len_2 = []
        self.shift_labels_2 = []
        for k in range(self.train_num):
            goal = self.mprompt[k]
            target = self.answers[k]

            conv_ = conv_2.copy()

            # print(text)
            if len(conv_.messages) > 0 and conv_.messages[-1][0] == conv_.roles[0] \
                    and conv_.messages[-1][1][-6:] == '</Img>':  # last message is image.
                conv_.messages[-1][1] = ' '.join([conv_.messages[-1][1], goal])
            else:
                conv_.append_message(conv_.roles[0], goal)
            conv_.append_message(conv_.roles[1], target)
            self.conv_2.append(conv_)

            embs, inputs_tokens = self.get_context_emb(self.model_3, conv_, image_list_2, flag=2)

            target_len_ = inputs_tokens.shape[1]
            self.target_len_2.append(target_len_)

            shift_labels_ = inputs_tokens[..., 1:].contiguous()
            self.shift_labels_2.append(shift_labels_)
        print('Finished Minigpt4(Llama2-7B) Initialization')

    def get_context_emb(self, model, conv, img_list, flag):
        prompt = conv.get_prompt()
        prompt_segs = prompt.split('<ImageHere>')

        seg_tokens = [
            model.llama_tokenizer(
                seg, return_tensors="pt", add_special_tokens=i == 0).to(self.device).input_ids
            # only add bos to the first seg
            for i, seg in enumerate(prompt_segs)
        ]

        inputs_tokens = []
        inputs_tokens.append(seg_tokens[0])
        if flag == 1:
            inputs_tokens.append(torch.from_numpy(np.ones((1,32))*(-200)).to(device) ) #for 224*224 num_Vtokens=32
        else:
            inputs_tokens.append(torch.from_numpy(np.ones((1, 64)) * (-200)).to(device))  # for 448*448 num_Vtokens=256
        inputs_tokens.append(seg_tokens[1])

        dtype = inputs_tokens[0].dtype
        inputs_tokens = torch.cat(inputs_tokens, dim=1).to(dtype)
        # print(inputs_tokens)
        # print(inputs_tokens.shape)
        seg_embs = [model.embed_tokens(seg_t) for seg_t in seg_tokens]

        mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]
        mixed_embs = torch.cat(mixed_embs, dim=1)
        return mixed_embs, inputs_tokens

    def forward(self, inp):
        images = inp[0]
        k = inp[1]

        image_emb_1, _ = self.model_1.encode_img(images)
        image_emb_2, _ = self.model_2.encode_img(images)
        image_emb_3, _ = self.model_3.encode_img(images)
        image_list_1 = []
        image_list_1.append(image_emb_1)
        image_list_2 = []
        image_list_2.append(image_emb_2)
        image_list_3 = []
        image_list_3.append(image_emb_3)


        loss_fct = nn.CrossEntropyLoss(ignore_index=-200)

        loss = 0
        conv_1 = self.conv_1[k]
        conv_2 = self.conv_2[k]
        target_len_1 = self.target_len_1[k]
        target_len_2 = self.target_len_2[k]
        shift_labels_1 = self.shift_labels_1[k]
        shift_labels_2 = self.shift_labels_2[k]

        embs_1, _ = self.get_context_emb(self.model_1, conv_1, image_list_1, 1)
        embs_2, _ = self.get_context_emb(self.model_2, conv_1, image_list_2, 1)
        embs_3, _ = self.get_context_emb(self.model_3, conv_2, image_list_3, 2)


        max_new_tokens = 300
        min_length = 1
        max_length = 2000

        current_max_len = embs_1.shape[1] + max_new_tokens
        if current_max_len - max_length > 0:
            print('Warning: The number of tokens in current conversation exceeds the max length. '
                  'The model will not see the contexts outside the range.')
        begin_idx = max(0, current_max_len - max_length)
        embs_1 = embs_1[:, begin_idx:]
        embs_2 = embs_2[:, begin_idx:]
        embs_3 = embs_3[:, begin_idx:]

        outputs_1 = self.model_1.llama_model(inputs_embeds=embs_1)
        outputs_2 = self.model_2.llama_model(inputs_embeds=embs_2)
        outputs_3 = self.model_3.llama_model(inputs_embeds=embs_3)
        logits_1 = outputs_1.logits
        logits_2 = outputs_2.logits
        logits_3 = outputs_3.logits


        lm_logits_1 = logits_1[:, :target_len_1, :]
        lm_logits_2 = logits_2[:, :target_len_1, :]
        lm_logits_3 = logits_3[:, :target_len_2, :]

        # Shift so that tokens < n predict n
        shift_logits_1 = lm_logits_1[..., :-1, :].contiguous()
        shift_logits_2 = lm_logits_2[..., :-1, :].contiguous()
        shift_logits_3 = lm_logits_3[..., :-1, :].contiguous()

        loss += loss_fct(shift_logits_1.view(-1, shift_logits_1.size(-1)), shift_labels_1.view(-1))
        loss += loss_fct(shift_logits_2.view(-1, shift_logits_2.size(-1)), shift_labels_1.view(-1))
        loss += loss_fct(shift_logits_3.view(-1, shift_logits_3.size(-1)), shift_labels_2.view(-1))
        return -loss




