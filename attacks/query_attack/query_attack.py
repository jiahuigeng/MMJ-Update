import argparse
import os
import random
import clip
import numpy as np
import torch
import torchvision
from PIL import Image
import copy
import torchvision.transforms.functional as TF
from tqdm import tqdm
import json
import pandas as pd
import multimodalmodels
import torchvision.transforms as transforms
import torchvision.utils as utils 

# Seed for everything
DEFAULT_RANDOM_SEED = 2024


class Query_Attack:
    def __init__(self, **kwargs):
        self.model_name = kwargs['model_name']
        self.model = getattr(multimodalmodels, self.model_name)()

        self.clip_img_model_vitb32, _ = clip.load("ViT-B/32", device=self.model.device, jit=False)
        self.transform = torchvision.transforms.Compose([
            torchvision.transforms.Lambda(lambda img: img.convert("RGB")),
            torchvision.transforms.Resize(size=(224, 224), interpolation=torchvision.transforms.InterpolationMode.BICUBIC, max_size=None, antialias='warn'),
            torchvision.transforms.ToTensor()
        ])

        self.alpha = kwargs['alpha']                
        self.epsilon = kwargs['epsilon']
        self.sigma = kwargs['sigma']
        self.steps = kwargs['steps']
        self.num_query = kwargs['num_query']
        self.image_path = kwargs['image_path']

        with open(kwargs['targets_path'], 'r', encoding='utf-8') as file:
            self.behavior_id_to_target = json.load(file)        

    def generate_test_cases(self, behaviors, save_dir, verbose=False):
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(os.path.join(save_dir, 'images'), exist_ok=True)     

        test_cases = {}

        for behavior_dict in tqdm(behaviors, total=len(behaviors)):
            behavior = behavior_dict['Behavior']
            behavior_id = behavior_dict['BehaviorID']

            target = self.behavior_id_to_target[behavior_id]
            if verbose:
                print(f"Behavior: {behavior}")
                print(f"Behavior ID: {behavior_id}")
                print(f"Target: {target}")                

            image_pil = Image.open(self.image_path)
            image = self.transform(image_pil)
            assert image.min() >=0 and image.max() <=1, "image_input values must be between 0 and 1"
            adv_image = self.attack(image, behavior, target)
            utils.save_image(adv_image, os.path.join(save_dir, 'images', f'{behavior_id}.png'))

        save_path = os.path.join(save_dir, 'Query_attack_test_cases.json')
        os.makedirs(os.path.dirname(save_path), exist_ok=True) if os.path.dirname(save_path) else None
        with open(save_path, 'w') as file:
            json.dump(test_cases, file, indent=2)        

    def seedEverything(seed=DEFAULT_RANDOM_SEED):
        random.seed(seed)
        os.environ['PYTHONHASHSEED'] = str(seed)
        np.random.seed(seed)        
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    def clip_text_emb(self, text):
        with torch.no_grad():
            clip_text_token = clip.tokenize(text, truncate=True).to(self.model.device)
            clip_text_embeddings = self.clip_img_model_vitb32.encode_text(clip_text_token)
            clip_text_embeddings = clip_text_embeddings / clip_text_embeddings.norm(dim=1, keepdim=True)
            return clip_text_embeddings.detach()

    def attack(self, image, behavior, target):

        image = image.to(self.model.device)

        delta = (8/256) * torch.randn(image.shape).clone().detach().to(self.model.device)
        tgt_text_embedding = self.clip_text_emb(target)
        len_gen_tokens = len(target.split())
        query_attack_result = 0
        best_answer = ""

        _, width, height = image.shape
        for step_idx in range(self.steps):
        # step 1. obtain purturbed images
            if step_idx == 0:
                image_repeat = image.repeat(self.num_query, 1, 1, 1)  # size = (num_query, 3, args.input_res, args.input_res)
                adv_text_embedding = torch.zeros((1, 512)).to(self.model.device)                    
            else:
                image_repeat      = adv_image_embedding_in_current_step.repeat(self.num_query, 1, 1, 1)  
                torch.cuda.empty_cache()

            #add noise
            query_noise            = torch.randn_like(image_repeat).sign() # Rademacher noise
            perturbed_image_repeat = torch.clamp(image_repeat + (self.sigma * query_noise)/256, 0.0, 1.0)  # size = (num_query x batch_size, 3, args.input_res, args.input_res)
            text_of_perturbed_imgs = []
            for item, tensor_image in enumerate(perturbed_image_repeat):
                with torch.no_grad():
                    pil_image = TF.to_pil_image(tensor_image)
                    text_of_sub_perturbed_imgs = self.model.query(pil_image, behavior)
                    gen_tokens = " ".join(text_of_sub_perturbed_imgs.split()[:len_gen_tokens])
                    text_of_perturbed_imgs.append(gen_tokens)            

            # step 2. estimate grad
            perturb_text_embeddings  =  self.clip_text_emb(text_of_perturbed_imgs)

            coefficient = torch.sum((perturb_text_embeddings - adv_text_embedding) * tgt_text_embedding, dim=-1)  # size = (num_query * batch_size)
            coefficient = coefficient.reshape(self.num_query, 1, 1, 1)
            query_noise = query_noise.reshape(self.num_query, 3, width, height)
            pseudo_gradient = 256 * coefficient * query_noise / self.sigma # size = (num_query, batch_size, 3, args.input_res, args.input_res)
            pseudo_gradient = pseudo_gradient.squeeze()
            pseudo_gradient = pseudo_gradient.mean(0) # size = (bs, 3, args.input_res, args.input_res)
            delta_data = torch.clamp(delta + self.alpha * torch.sign(pseudo_gradient)/256, min=-self.epsilon/256, max=self.epsilon/256)
            delta.data = delta_data


            adv_image_emb_in_current_step_temp = torch.clamp(image + delta, 0.0, 1.0)
            adv_image_in_current_step_temp = TF.to_pil_image(adv_image_emb_in_current_step_temp)
         

            with torch.no_grad():
                text_of_adv_image_in_current_step = self.model.query(adv_image_in_current_step_temp, behavior)
                truncated_text_of_adv_image_in_current_step =  " ".join(text_of_adv_image_in_current_step.split()[:len_gen_tokens])
                adv_text_embedding = self.clip_text_emb(truncated_text_of_adv_image_in_current_step)
                adv_txt_sim_score = torch.mean(torch.sum(adv_text_embedding * tgt_text_embedding, dim=1)).item()

                if adv_txt_sim_score > query_attack_result:
                    adv_image_embedding_in_current_step = adv_image_emb_in_current_step_temp
                    query_attack_result = adv_txt_sim_score
                    best_answer = text_of_adv_image_in_current_step
        
        return adv_image_embedding_in_current_step











